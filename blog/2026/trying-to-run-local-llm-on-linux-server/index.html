<!doctype html><html lang=en-us dir=ltr><head><script>(function(){var e=localStorage.getItem("theme");e&&document.documentElement.setAttribute("data-theme",e)})()</script><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=5"><title>Trying to run local LLM on linux server | Martin Proks</title><link rel=stylesheet href=/css/style.min.d523984b0917fb197985865addf99d7b456524ee84d66062293622af26027502.css integrity="sha256-1SOYSwkX+xl5hYZa3fmde0VlJO6E1mBiKTYiryYCdQI=" crossorigin=anonymous><link rel=stylesheet href=/css/custom.min.7305c69fb0969c75d5ef4ded7b6ac1720f3a74479d645a355d5dcae17a759d4d.css integrity="sha256-cwXGn7CWnHXV703te2rBcg86dEedZFo1XV3K4Xp1nU0=" crossorigin=anonymous><link rel=stylesheet href=/css/pygments-friendly.min.28135e6c0da5803b920fa8df2ca818e8bffa680293762a228fe96c10634a9662.css integrity="sha256-KBNebA2lgDuSD6jfLKgY6L/6aAKTdioij+lsEGNKlmI=" crossorigin=anonymous><script defer src=/js/main.a40195b6f9b3a8d8e9909bc9d54bd2e00b6094ae3afbe68734b5b111776e632e.js integrity="sha256-pAGVtvmzqNjpkJvJ1UvS4AtglK46++aHNLWxEXduYy4=" crossorigin=anonymous></script><script defer src=https://cloud.umami.is/script.js data-website-id=2abc48d3-fb15-4188-8a7f-8ce1fc758770></script></head><body><header><div class=site-header><a class=title href=/><h1>Martin Proks</h1></a><nav class=site-nav><a href=/>Home</a> <a href=/blog/>Blog</a> <a href=/projects>Projects</a> <a href=/about>About</a> <a href=/tags/>Tags</a><button class=theme-toggle type=button aria-label="Toggle theme" title="Toggle theme">
<svg class="icon-sun" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg>
<svg class="icon-moon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg></button></nav></div></header><main><article class=post><header class=post-header><h1 class=post-title>Trying to run local LLM on linux server</h1><p class=post-meta><time datetime=2026-02-08>08 Feb, 2026</time></p></header><div class=post-content><p>I&rsquo;ve been testing a local LLM on my trusty MacBook Pro M4 16GB using <a href=https://ollama.com>ollama</a>. However, this configuration is limited to only smaller models with quantization usually around 4 bits. Luckily enough, I have access to a linux server with some GPUs to play with. When it comes to running LLM locally with CUDA, based on my reading the options are either ollama, <a href=https://vllm.ai/>vLLM</a> or <a href=https://github.com/ggml-org/llama.cpp>llama.cpp</a>.</p><p>I wanted to try something new so I started first with vllm and boy oh boy, installing this was a nightmare. The latest wheels for vLLM (<a href=https://github.com/vllm-project/vllm/releases/tag/v0.15.1>v0.15.1</a>) are compiled for CUDA 13 (I have only access to CUDA 12 on the HPC). I started first with recommended approach using <a href=https://docs.astral.sh/uv/>uv</a> and <code>--torch-backend=auto</code>. After a while I am hitting the first barrier also described in <a href=https://github.com/vllm-project/vllm/issues/7785>issue #7785</a>.</p><div class=codeblock><div class=codeblock-actions><button type=button class="codeblock-action codeblock-action--wrap" data-wrap-toggle data-icon-button aria-pressed=false title="Enable soft wrap" data-wrap-title="Enable soft wrap" data-nowrap-title="Disable soft wrap">
<svg viewBox="0 0 24 24" aria-hidden="true" focusable="false"><path d="M4 6h16M4 10h12a4 4 0 010 8h-2" stroke="currentColor" stroke-width="2" fill="none" stroke-linecap="round"/><path d="M14 18l-2-2m2 2-2 2" stroke="currentColor" stroke-width="2" fill="none" stroke-linecap="round" stroke-linejoin="round"/></svg>
</button>
<button type=button class="codeblock-action codeblock-action--copy" data-copy-code data-icon-button title="Copy code" data-copied-title=COPIED>
<svg class="icon-copy" viewBox="0 0 24 24" aria-hidden="true" focusable="false"><rect x="9" y="9" width="13" height="13" rx="2" ry="2" stroke="currentColor" stroke-width="2" fill="none"/><path d="M5 15H4a2 2 0 01-2-2V4a2 2 0 012-2h9a2 2 0 012 2v1" stroke="currentColor" stroke-width="2" fill="none" stroke-linecap="round"/></svg>
<svg class="icon-check" viewBox="0 0 24 24" aria-hidden="true" focusable="false"><path d="M20 6 9 17l-5-5" stroke="currentColor" stroke-width="2" fill="none" stroke-linecap="round" stroke-linejoin="round"/></svg></button></div><div class=highlight><pre tabindex=0 class=chroma><code><span class=line><span class=cl>&#39;xxxx/.cache/uv/builds-v0/.tmpMou2on/bin/ninja&#39; &#39;--version&#39;
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        failed with:
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>         no such file or directory</span></span></code></pre></div></div><p>Ok, <code>ninja</code> is part of the <a href=https://github.com/vllm-project/vllm/blob/785cf28ffffeddeecf79018074a222b7c5938f9c/requirements/build.txt#L3>requirements</a>, so this shouldn&rsquo;t be an issue. No problem, I have ninja available as a module on the HPC, I&rsquo;ll just load it and restart. To no avail, I run <code>uv cache clean</code>, restart the process. I&rsquo;m getting nowhere, time to build from source. So I git clone, setup the venv and run <code>uv pip install --editable .</code>. This time a new problem occurs (<a href=https://github.com/vllm-project/vllm/issues/23062>issue #23062</a>)</p><div class=codeblock><div class=codeblock-actions><button type=button class="codeblock-action codeblock-action--wrap" data-wrap-toggle data-icon-button aria-pressed=false title="Enable soft wrap" data-wrap-title="Enable soft wrap" data-nowrap-title="Disable soft wrap">
<svg viewBox="0 0 24 24" aria-hidden="true" focusable="false"><path d="M4 6h16M4 10h12a4 4 0 010 8h-2" stroke="currentColor" stroke-width="2" fill="none" stroke-linecap="round"/><path d="M14 18l-2-2m2 2-2 2" stroke="currentColor" stroke-width="2" fill="none" stroke-linecap="round" stroke-linejoin="round"/></svg>
</button>
<button type=button class="codeblock-action codeblock-action--copy" data-copy-code data-icon-button title="Copy code" data-copied-title=COPIED>
<svg class="icon-copy" viewBox="0 0 24 24" aria-hidden="true" focusable="false"><rect x="9" y="9" width="13" height="13" rx="2" ry="2" stroke="currentColor" stroke-width="2" fill="none"/><path d="M5 15H4a2 2 0 01-2-2V4a2 2 0 012-2h9a2 2 0 012 2v1" stroke="currentColor" stroke-width="2" fill="none" stroke-linecap="round"/></svg>
<svg class="icon-check" viewBox="0 0 24 24" aria-hidden="true" focusable="false"><path d="M20 6 9 17l-5-5" stroke="currentColor" stroke-width="2" fill="none" stroke-linecap="round" stroke-linejoin="round"/></svg></button></div><div class=highlight><pre tabindex=0 class=chroma><code><span class=line><span class=cl>Pytorch version 2.9.1 expected for CUDA build, saw 2.4.0 instead.</span></span></code></pre></div></div><p>Alright, wrong Pytorch version let me install it manually first <code>uv pip install torch==2.9.1</code> and restart. No change, same issue. Ok, probably it downgrades back, so I read the documentation further and find I have to run <code>python use_existing_torch.py</code> when enforcing custom Pytorch version. Re-run again and this time</p><div class=codeblock><div class=codeblock-actions><button type=button class="codeblock-action codeblock-action--wrap" data-wrap-toggle data-icon-button aria-pressed=false title="Enable soft wrap" data-wrap-title="Enable soft wrap" data-nowrap-title="Disable soft wrap">
<svg viewBox="0 0 24 24" aria-hidden="true" focusable="false"><path d="M4 6h16M4 10h12a4 4 0 010 8h-2" stroke="currentColor" stroke-width="2" fill="none" stroke-linecap="round"/><path d="M14 18l-2-2m2 2-2 2" stroke="currentColor" stroke-width="2" fill="none" stroke-linecap="round" stroke-linejoin="round"/></svg>
</button>
<button type=button class="codeblock-action codeblock-action--copy" data-copy-code data-icon-button title="Copy code" data-copied-title=COPIED>
<svg class="icon-copy" viewBox="0 0 24 24" aria-hidden="true" focusable="false"><rect x="9" y="9" width="13" height="13" rx="2" ry="2" stroke="currentColor" stroke-width="2" fill="none"/><path d="M5 15H4a2 2 0 01-2-2V4a2 2 0 012-2h9a2 2 0 012 2v1" stroke="currentColor" stroke-width="2" fill="none" stroke-linecap="round"/></svg>
<svg class="icon-check" viewBox="0 0 24 24" aria-hidden="true" focusable="false"><path d="M20 6 9 17l-5-5" stroke="currentColor" stroke-width="2" fill="none" stroke-linecap="round" stroke-linejoin="round"/></svg></button></div><div class=highlight><pre tabindex=0 class=chroma><code><span class=line><span class=cl>Pytorch version 2.4.0 expected for CUDA build, saw 2.9.1 instead.</span></span></code></pre></div></div><p>What the *** is happening here. I&rsquo;ve spend few more hours on this and then decided that unless I upgrade to CUDA 13 this is going nowhere. So I abandon this approach for and switch my focus to llama.cpp.</p><p>After this experience I wasn&rsquo;t sure I would be successful with llama.cpp, but after I was determined to make this work. Surprisingly the only issue I encountered was that I was using old version of gcc (v9.x) when compiling from source. With simple upgrade to gcc v11, I&rsquo;ve successfully managed to compile the package for CUDA with</p><div class=codeblock><div class=codeblock-actions><button type=button class="codeblock-action codeblock-action--wrap" data-wrap-toggle data-icon-button aria-pressed=false title="Enable soft wrap" data-wrap-title="Enable soft wrap" data-nowrap-title="Disable soft wrap">
<svg viewBox="0 0 24 24" aria-hidden="true" focusable="false"><path d="M4 6h16M4 10h12a4 4 0 010 8h-2" stroke="currentColor" stroke-width="2" fill="none" stroke-linecap="round"/><path d="M14 18l-2-2m2 2-2 2" stroke="currentColor" stroke-width="2" fill="none" stroke-linecap="round" stroke-linejoin="round"/></svg>
</button>
<button type=button class="codeblock-action codeblock-action--copy" data-copy-code data-icon-button title="Copy code" data-copied-title=COPIED>
<svg class="icon-copy" viewBox="0 0 24 24" aria-hidden="true" focusable="false"><rect x="9" y="9" width="13" height="13" rx="2" ry="2" stroke="currentColor" stroke-width="2" fill="none"/><path d="M5 15H4a2 2 0 01-2-2V4a2 2 0 012-2h9a2 2 0 012 2v1" stroke="currentColor" stroke-width="2" fill="none" stroke-linecap="round"/></svg>
<svg class="icon-check" viewBox="0 0 24 24" aria-hidden="true" focusable="false"><path d="M20 6 9 17l-5-5" stroke="currentColor" stroke-width="2" fill="none" stroke-linecap="round" stroke-linejoin="round"/></svg></button></div><div class=highlight><pre tabindex=0 class=chroma><code><span class=line><span class=cl>module load cuda/12.8 cudnn/9.18.0.77_cuda12 cmake/4.0.3 gcc/11.2.0
</span></span><span class=line><span class=cl>git clone https://github.com/ggml-org/llama.cpp
</span></span><span class=line><span class=cl><span class=nb>cd</span> llama.cpp
</span></span><span class=line><span class=cl>cmake -B build -DGGML_CUDA<span class=o>=</span>ON
</span></span><span class=line><span class=cl>cmake --build build --config Release -- -j <span class=m>30</span></span></span></code></pre></div></div><p>I&rsquo;ve tested it using the <a href=https://github.com/ggml-org/llama.cpp/discussions/16938>new WebUI</a> and it worked without any issues</p><div class=codeblock><div class=codeblock-actions><button type=button class="codeblock-action codeblock-action--wrap" data-wrap-toggle data-icon-button aria-pressed=false title="Enable soft wrap" data-wrap-title="Enable soft wrap" data-nowrap-title="Disable soft wrap">
<svg viewBox="0 0 24 24" aria-hidden="true" focusable="false"><path d="M4 6h16M4 10h12a4 4 0 010 8h-2" stroke="currentColor" stroke-width="2" fill="none" stroke-linecap="round"/><path d="M14 18l-2-2m2 2-2 2" stroke="currentColor" stroke-width="2" fill="none" stroke-linecap="round" stroke-linejoin="round"/></svg>
</button>
<button type=button class="codeblock-action codeblock-action--copy" data-copy-code data-icon-button title="Copy code" data-copied-title=COPIED>
<svg class="icon-copy" viewBox="0 0 24 24" aria-hidden="true" focusable="false"><rect x="9" y="9" width="13" height="13" rx="2" ry="2" stroke="currentColor" stroke-width="2" fill="none"/><path d="M5 15H4a2 2 0 01-2-2V4a2 2 0 012-2h9a2 2 0 012 2v1" stroke="currentColor" stroke-width="2" fill="none" stroke-linecap="round"/></svg>
<svg class="icon-check" viewBox="0 0 24 24" aria-hidden="true" focusable="false"><path d="M20 6 9 17l-5-5" stroke="currentColor" stroke-width="2" fill="none" stroke-linecap="round" stroke-linejoin="round"/></svg></button></div><div class=highlight><pre tabindex=0 class=chroma><code><span class=line><span class=cl><span class=nb>export</span> <span class=nv>CUDA_VISIBLE_DEVICES</span><span class=o>=</span>2,3
</span></span><span class=line><span class=cl>llama-server -m glm-4.7-flash-claude-4.5-opus.f16.gguf --port <span class=m>8080</span> --ctx-size <span class=m>0</span> --jinja -ub <span class=m>2048</span> -b <span class=m>2048</span></span></span></code></pre></div></div><p>My next plan is to connect it to <a href=https://opencode.ai>opencode</a> and experiment more with the agentic mode.</p><p>I&rsquo;m still not quite sure what I did wrong with the vLLM setup, but at this point llama.cpp seems to work great. If you&rsquo;ve hit the same issues as I did during installation and found a way how to resolve it, do let me know by submitting <a href=https://github.com/matq007/matq007.github.io/issues>an issue here</a>.</p></div><footer class=post-footer><div class=post-footer-row><div class=post-tags><a rel=nofollow href=/tags/blog%2Fnotes>#blog/notes</a><a rel=nofollow href=/tags/llm>#llm</a></div></div></footer></article></main><footer><nav class=footer-links aria-label="Social links"><a href=/index.xml title=RSS aria-label=RSS>RSS</a><a href=https://github.com/matq007 title=GitHub aria-label=GitHub>GitHub</a><a href=https://x.com/mproksik title=X aria-label=X>X</a><a href=https://www.linkedin.com/in/martinproks title=LinkedIn aria-label=LinkedIn>LinkedIn</a></nav><p>&copy; 2026 Martin Proks â€” Powered by <a href=https://gohugo.io/ target=_blank>Hugo</a> & <a href=https://github.com/binbinsh/hugo-trainsh target=_blank>hugo-trainsh</a></p></footer><script>document.addEventListener("DOMContentLoaded",function(){var e,t,n=document.querySelector(".theme-toggle");if(n){function s(){var e=localStorage.getItem("theme");return e?e:window.matchMedia("(prefers-color-scheme: dark)").matches?"dark":"light"}n.addEventListener("click",function(){var t=s(),e=t==="dark"?"light":"dark";document.documentElement.setAttribute("data-theme",e),localStorage.setItem("theme",e)})}e=document.querySelector(".lang-dropdown"),e&&(t=e.querySelector(".lang-toggle"),t.addEventListener("click",function(n){n.stopPropagation(),e.classList.toggle("open"),t.setAttribute("aria-expanded",e.classList.contains("open"))}),document.addEventListener("click",function(n){e.contains(n.target)||(e.classList.remove("open"),t.setAttribute("aria-expanded","false"))}))})</script></body></html>