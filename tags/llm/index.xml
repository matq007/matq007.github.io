<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Llm on Martin Proks</title><link>/tags/llm/</link><description>Recent content in Llm on Martin Proks</description><generator>Hugo</generator><language>en-us</language><lastBuildDate>Sun, 22 Feb 2026 08:00:00 +0000</lastBuildDate><atom:link href="/tags/llm/index.xml" rel="self" type="application/rss+xml"/><item><title>AlexsJones/llmfit</title><link>/blog/2026/llmfit/</link><pubDate>Sun, 22 Feb 2026 08:00:00 +0000</pubDate><guid>/blog/2026/llmfit/</guid><description>&lt;p&gt;&lt;a href="https://github.com/AlexsJones/llmfit"&gt;llmfit&lt;/a&gt; written by Alex Jones can help you figure out which LLM model best fits your gear setup. Versatile tools with many settings.&lt;/p&gt;
&lt;p&gt;Written in Rut, can be installed with &lt;code&gt;cargo&lt;/code&gt; or &lt;code&gt;brew&lt;/code&gt;&lt;/p&gt;
&lt;div class="codeblock"&gt;
 &lt;div class="codeblock-actions"&gt;
 &lt;button type="button"
 class="codeblock-action codeblock-action--wrap"
 data-wrap-toggle
 data-icon-button
 aria-pressed="false"
 title="Enable soft wrap"
 data-wrap-title="Enable soft wrap"
 data-nowrap-title="Disable soft wrap"&gt;
 &lt;svg viewBox="0 0 24 24" aria-hidden="true" focusable="false"&gt;
 &lt;path d="M4 6h16M4 10h12a4 4 0 0 1 0 8h-2" stroke="currentColor" stroke-width="2" fill="none" stroke-linecap="round"/&gt;
 &lt;path d="M14 18l-2-2m2 2l-2 2" stroke="currentColor" stroke-width="2" fill="none" stroke-linecap="round" stroke-linejoin="round"/&gt;
 &lt;/svg&gt;
 &lt;/button&gt;

 &lt;button type="button"
 class="codeblock-action codeblock-action--copy"
 data-copy-code
 data-icon-button
 title="Copy code"
 data-copied-title="COPIED"&gt;
 &lt;svg class="icon-copy" viewBox="0 0 24 24" aria-hidden="true" focusable="false"&gt;
 &lt;rect x="9" y="9" width="13" height="13" rx="2" ry="2" stroke="currentColor" stroke-width="2" fill="none"&gt;&lt;/rect&gt;
 &lt;path d="M5 15H4a2 2 0 0 1-2-2V4a2 2 0 0 1 2-2h9a2 2 0 0 1 2 2v1" stroke="currentColor" stroke-width="2" fill="none" stroke-linecap="round"&gt;&lt;/path&gt;
 &lt;/svg&gt;
 &lt;svg class="icon-check" viewBox="0 0 24 24" aria-hidden="true" focusable="false"&gt;
 &lt;path d="M20 6L9 17l-5-5" stroke="currentColor" stroke-width="2" fill="none" stroke-linecap="round" stroke-linejoin="round"&gt;&lt;/path&gt;
 &lt;/svg&gt;
 &lt;/button&gt;
 &lt;/div&gt;&lt;div class="highlight"&gt;&lt;pre tabindex="0" class="chroma"&gt;&lt;code&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;brew tap AlexsJones/llmfit
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;brew install llmfit&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;and then simply running&lt;/p&gt;</description></item><item><title>Integrating Jan with opencode</title><link>/blog/2026/integrating-jan-with-opencode/</link><pubDate>Sun, 22 Feb 2026 08:00:00 +0000</pubDate><guid>/blog/2026/integrating-jan-with-opencode/</guid><description>&lt;p&gt;This weekend I went back to test again &lt;a href="https://www.jan.ai/"&gt;Jan.ai&lt;/a&gt; for running LLM models locally on my MacBook. I do like the simple interface and already build in web-search using &lt;a href="https://exa.ai/"&gt;exa&lt;/a&gt;. From my crude testing I concluded &lt;a href="https://huggingface.co/mlx-community"&gt;MLX optimized models&lt;/a&gt; run faster compared to llama.cpp and I wanted to test it further with opencode. Since I haven&amp;rsquo;t found official documentation on how to integrate &lt;code&gt;mlx-lm&lt;/code&gt; with opencode I&amp;rsquo;m providing links and settings on how to achieve it.&lt;/p&gt;</description></item><item><title>Integrating Jan.ai with opencode</title><link>/blog/2026/integrating-jan.ai-with-opencode/</link><pubDate>Sun, 22 Feb 2026 08:00:00 +0000</pubDate><guid>/blog/2026/integrating-jan.ai-with-opencode/</guid><description>&lt;p&gt;This weekend I went back to test again &lt;a href="https://www.jan.ai/"&gt;Jan.ai&lt;/a&gt; for running LLM models locally on my MacBook. I do like the simple interface and already build in web-search using &lt;a href="https://exa.ai/"&gt;exa&lt;/a&gt;. From my crude testing I concluded &lt;a href="https://huggingface.co/mlx-community"&gt;MLX optimized models&lt;/a&gt; run faster compared to llama.cpp and I wanted to test it further with opencode. Since I haven&amp;rsquo;t found official documentation on how to integrate &lt;code&gt;mlx-lm&lt;/code&gt; with opencode I&amp;rsquo;m providing links and settings on how to achieve it.&lt;/p&gt;</description></item><item><title>LLM Visualization</title><link>/blog/2026/llm-visualization/</link><pubDate>Sun, 08 Feb 2026 08:00:00 +0000</pubDate><guid>/blog/2026/llm-visualization/</guid><description>&lt;p&gt;&lt;a href="https://bbycroft.net"&gt;Brendan Bycroft&lt;/a&gt; created &lt;a href="https://bbycroft.net/llm"&gt;an online app&lt;/a&gt; which visually walks you through architecture and steps of LLM models (GPT-2 small, nano-gpt, GT2 (XL) or GPT-3).&lt;/p&gt;</description></item><item><title>Trying to run local LLM on linux server</title><link>/blog/2026/trying-to-run-local-llm-on-linux-server/</link><pubDate>Sun, 08 Feb 2026 08:00:00 +0000</pubDate><guid>/blog/2026/trying-to-run-local-llm-on-linux-server/</guid><description>&lt;p&gt;I&amp;rsquo;ve been testing a local LLM on my trusty MacBook Pro M4 16GB using &lt;a href="https://ollama.com"&gt;ollama&lt;/a&gt;. However, this configuration is limited to only smaller models with quantization usually around 4 bits. Luckily enough, I have access to a linux server with some GPUs to play with. When it comes to running LLM locally with CUDA, based on my reading the options are either ollama, &lt;a href="https://vllm.ai/"&gt;vLLM&lt;/a&gt; or &lt;a href="https://github.com/ggml-org/llama.cpp"&gt;llama.cpp&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;I wanted to try something new so I started first with vllm and boy oh boy, installing this was a nightmare. The latest wheels for vLLM (&lt;a href="https://github.com/vllm-project/vllm/releases/tag/v0.15.1"&gt;v0.15.1&lt;/a&gt;) are compiled for CUDA 13 (I have only access to CUDA 12 on the HPC). I started first with recommended approach using &lt;a href="https://docs.astral.sh/uv/"&gt;uv&lt;/a&gt; and &lt;code&gt;--torch-backend=auto&lt;/code&gt;. After a while I am hitting the first barrier also described in &lt;a href="https://github.com/vllm-project/vllm/issues/7785"&gt;issue #7785&lt;/a&gt;.&lt;/p&gt;</description></item><item><title>Epsilon in AdamW matters</title><link>/blog/2026/epsilon-in-adamw-matters/</link><pubDate>Sat, 24 Jan 2026 12:00:00 +0000</pubDate><guid>/blog/2026/epsilon-in-adamw-matters/</guid><description>&lt;p&gt;When training LLM, &lt;a href="https://sifal.social"&gt;Sifal&lt;/a&gt; shows that switching from default &lt;code&gt;eps=1e-8&lt;/code&gt; to &lt;code&gt;eps=1e-10&lt;/code&gt; in AdamW optimizer can lead to better results. He showcases this on his toy example where default epsilon oscillates when searching for local minima compared to proposed one. However, this only applies when training is &lt;strong&gt;NOT&lt;/strong&gt; done with &lt;a href="https://docs.pytorch.org/docs/stable/notes/numerical_accuracy.html#reduced-precision-reduction-for-fp16-and-bf16-gemms"&gt;half-precision&lt;/a&gt;.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Klioui, S. (2026). The Epsilon Trap: When Adam Stops Being Adam. Sifal Klioui Blog. &lt;a href="https://sifal.social/posts/The-Epsilon-Trap-When-Adam-Stops-Being-Adam/"&gt;link&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;</description></item><item><title>microsoft/BitNet</title><link>/blog/2026/bitnet/</link><pubDate>Sat, 24 Jan 2026 12:00:00 +0000</pubDate><guid>/blog/2026/bitnet/</guid><description>&lt;p&gt;A new paradigm shift from Microsoft called &lt;a href="https://github.com/microsoft/BitNet"&gt;BitNet&lt;/a&gt;, where the objective is to reduce speed and footprint of LLM models, by storing the LLM weights as 1bit. This is done during training, compared to quantization which fixes the weights post training. Authors report promising benchmarking with &lt;em&gt;BitNet b1.58 2B&lt;/em&gt; using only &lt;strong&gt;400MB&lt;/strong&gt; RAM and 29ms latency. It will be interesting to see where we will see these types of models deployed in future. Targets could be phones, IoT or privacy focused closed offline systems. For more in-depth details, I highly recommend watching &lt;a href="https://www.youtube.com/watch?v=WBm0nyDkVYM"&gt;this&lt;/a&gt; great explanation by &lt;a href="https://www.patreon.com/JuliaTurc"&gt;Julia Turc&lt;/a&gt;.&lt;/p&gt;</description></item><item><title>Week 22 (25-30/05/2025)</title><link>/blog/2025/2025-05-30/</link><pubDate>Fri, 30 May 2025 00:00:00 +0000</pubDate><guid>/blog/2025/2025-05-30/</guid><description>&lt;h2 id="-it"&gt;
 ðŸ‘¾ IT
&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Simon&amp;rsquo;s initial impression of Devstal from Mistral (&lt;a href="https://simonwillison.net/2025/May/21/devstral"&gt;link&lt;/a&gt;).&lt;/li&gt;
&lt;li&gt;Namanyay provides very simple yet convincing argument about how to leverage the AI
landscape in your day-to-day dev life. It&amp;rsquo;s not perfect, but it can be simplify
some tasks, you just need to figure out which ones (&lt;a href="https://nmn.gl/blog/ai-scam"&gt;link&lt;/a&gt;).&lt;/li&gt;
&lt;li&gt;Cloudflare proposes HTTP message signature to simplify how to deal with ai bots/crawlers (&lt;a href="https://blog.cloudflare.com/web-bot-auth/"&gt;link&lt;/a&gt;).&lt;/li&gt;
&lt;li&gt;Oddysey introduced model which generates AI video with real-time interaction
from user (&lt;a href="https://techcrunch.com/2025/05/28/odysseys-new-ai-model-streams-3d-interactive-worlds/"&gt;link&lt;/a&gt;).&lt;/li&gt;
&lt;li&gt;MinIO moves many features behind paywall &lt;a href="https://github.com/minio/object-browser/pull/3509"&gt;minio/object-browser&lt;/a&gt;.
&lt;ul&gt;
&lt;li&gt;Alternative switch: &lt;a href="https://garagehq.deuxfleurs.fr"&gt;Garage&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Nice tips &amp;amp; tricks on running LLM locally by Kiran (&lt;a href="https://blog.nilenso.com/blog/2025/05/06/local-llm-setup/"&gt;link&lt;/a&gt;).&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="science"&gt;
 ðŸ”¬Science
&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;FDA pushes initiative to replace animal models for pre-clinical studies by
embracing organ-on-a chip or in silico modeling if it offers to be a viable
alternative(&lt;a href="https://www.nature.com/articles/s41587-025-02690-0"&gt;link&lt;/a&gt;).&lt;/li&gt;
&lt;li&gt;More bones found in the &amp;ldquo;River of Death&amp;rdquo; in Alberta, Canada provides great
resource to study Pachyrhinosaurus and other dinosaurs (&lt;a href="https://www.bbc.com/news/articles/c0k3x8lmje1o"&gt;link&lt;/a&gt;).&lt;/li&gt;
&lt;li&gt;New #podcast series A Coffee with CompBio (&lt;a href="https://podcast.ausha.co/a-coffee-with-compbio"&gt;link&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.nature.com/articles/s41467-025-59926-5"&gt;Zeng et al., 2025&lt;/a&gt; develops
CellFM model using ERetNet paired with LoRA.&lt;/li&gt;
&lt;li&gt;Tackling how to extract number of sequenced cells from papers using LLM by Svensson (&lt;a href="https://www.nxn.se/p/evaluating-automatic-cell-number"&gt;link&lt;/a&gt;).&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.biorxiv.org/content/10.1101/2025.05.23.655749v1.full"&gt;Sikkema et al., 2025 BiorXiv&lt;/a&gt; developed MapQC
to estimate fitness of mapping between reference and query
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Energy distance metric (E-distance)&lt;/em&gt; measures shift between two distributions (signal-to-noise ratio)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="-links"&gt;
 ðŸ”— Links
&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Internet Archive launches Youtube channel where they stream how they scan material
using high-resolution camera, use automated software for cropping and OCR conversion
and archive it afterward on microfiche cards (&lt;a href="https://www.theverge.com/news/672682/internet-archive-microfiche-lo-fi-beats-channel"&gt;link&lt;/a&gt;).&lt;/li&gt;
&lt;li&gt;Faster Github Actiosn from Blacksmith company with their transparency on security layers (&lt;a href="https://www.blacksmith.sh/blog/security"&gt;link&lt;/a&gt;).&lt;/li&gt;
&lt;li&gt;Once provide a phone number it will find its way to the interwebs &lt;a href="https://danq.me/2025/05/21/google-shared-my-phone-number/"&gt;(link&lt;/a&gt;).&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="-tools"&gt;
 ðŸ”§ Tools
&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/simonw/llm"&gt;simonw/llm&lt;/a&gt;: LLM from terminal simply.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/awslabs/mountpoint-s3"&gt;awslabs/mountpoint-s3&lt;/a&gt;: Mount s3 bucket which is POSIX compliant.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/antinomyhq/forge"&gt;antinomyhq/forge&lt;/a&gt;: AI assistant directly in terminal.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/facebookincubator/cinder"&gt;facebookincubator/cinder&lt;/a&gt;: Meta&amp;rsquo;s version of CPython focused on speed performance.&lt;/li&gt;
&lt;/ul&gt;</description></item></channel></rss>